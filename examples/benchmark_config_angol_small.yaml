# DeepOBS Benchmark Configuration - Angol on Small to Medium Problems
# Test problems from small to large (up to ~20M parameters)

# Global settings
global:
  # Output directory for all results
  output_dir: "./results"

  # Random seed for reproducibility
  random_seed: 42

  # Default number of epochs
  num_epochs: 20

  # Default batch size
  batch_size: 128

  # Training log interval (log every N batches)
  train_log_interval: 100

  # Print training iteration info
  print_train_iter: false

  # Data directory (where datasets are stored)
  data_dir: null  # null = use default

# Test problems to benchmark (21 problems)
test_problems:
  # ========================================
  # MNIST problems (4 problems, 7.8K-3M params)
  # ========================================
  - name: mnist_logreg
    num_epochs: 5
    batch_size: 128

  - name: mnist_mlp
    num_epochs: 10
    batch_size: 128

  - name: mnist_2c2d
    num_epochs: 20
    batch_size: 128

  # Note: VAE dimension bug fixed, but may have DataLoader issues on macOS
  - name: mnist_vae
    num_epochs: 30
    batch_size: 128

  # ========================================
  # Fashion-MNIST problems (4 problems, 7.8K-3M params)
  # ========================================
  - name: fmnist_logreg
    num_epochs: 5
    batch_size: 128

  - name: fmnist_mlp
    num_epochs: 10
    batch_size: 128

  - name: fmnist_2c2d
    num_epochs: 20
    batch_size: 128

  # Note: VAE dimension bug fixed, but may have DataLoader issues on macOS
  - name: fmnist_vae
    num_epochs: 30
    batch_size: 128

  # ========================================
  # CIFAR-10 problems (1 problem, 1.4M params)
  # Note: VGG16/19 excluded (>10M params)
  # ========================================
  - name: cifar10_3c3d
    num_epochs: 50
    batch_size: 128

  # ========================================
  # CIFAR-100 problems (5 problems, 1.4M-20M params)
  # ========================================
  - name: cifar100_3c3d
    num_epochs: 50
    batch_size: 128

  - name: cifar100_allcnnc
    num_epochs: 50
    batch_size: 256

  - name: cifar100_wrn404
    num_epochs: 50
    batch_size: 128

  - name: cifar100_vgg16
    num_epochs: 50
    batch_size: 128

  - name: cifar100_vgg19
    num_epochs: 50
    batch_size: 128

  # ========================================
  # SVHN problems (2 problems, 1.4M-2.7M params)
  # ========================================
  - name: svhn_3c3d
    num_epochs: 40
    batch_size: 128

  - name: svhn_wrn164
    num_epochs: 40
    batch_size: 128

  # ========================================
  # Text generation problems (1 problem, ~500K params)
  # ========================================
  - name: textgen
    num_epochs: 30
    batch_size: 64

  # ========================================
  # Synthetic optimization problems (4 problems, 2-100 params)
  # ========================================
  - name: quadratic_deep
    num_epochs: 100
    batch_size: 128

  - name: two_d_rosenbrock
    num_epochs: 100
    batch_size: 128

  - name: two_d_beale
    num_epochs: 100
    batch_size: 128

  - name: two_d_branin
    num_epochs: 100
    batch_size: 128

# Optimizers to benchmark
optimizers:
  # Angol optimizer with standard hyperparameters
  - name: Angol
    optimizer_class: angol.Angol
    learning_rate: 0.001
    lr_factor: 0.1

# Problem-specific optimizer overrides
overrides:
  # Logistic regression problems need higher learning rate
  mnist_logreg:
    Angol:
      learning_rate: 0.01

  fmnist_logreg:
    Angol:
      learning_rate: 0.01

  # Synthetic problems might benefit from higher learning rate
  quadratic_deep:
    Angol:
      learning_rate: 0.01

  two_d_rosenbrock:
    Angol:
      learning_rate: 0.01

  two_d_beale:
    Angol:
      learning_rate: 0.01

  two_d_branin:
    Angol:
      learning_rate: 0.01
